{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import signal\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import pyeda.inter as inter\n",
    "import pyeda.boolalg as boolalg\n",
    "from tqdm.notebook import trange\n",
    "from graphviz import Digraph, Source\n",
    "from IPython.display import SVG, HTML, display\n",
    "from pprint import pprint\n",
    "\n",
    "infty = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class\n",
    "### ODD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODD():\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        self.cache = dict()\n",
    "        self.bdd = None\n",
    "        self.svg_graph = None\n",
    "        self.eda_vars = dict() # only for restrict bdd\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(boolalg.bdd.bdd2expr(self.root.eda_expr))\n",
    "\n",
    "    def display_expr(self):\n",
    "        if self.bdd is not None:\n",
    "            dot_odd = self.bdd.to_dot()\n",
    "            display(Source(dot_odd))\n",
    "\n",
    "    def display_graph(self):\n",
    "        if not self.svg_graph:\n",
    "            self.make_graph()\n",
    "        display(SVG(self.svg_graph))\n",
    "                \n",
    "\n",
    "class Node:\n",
    "    def __init__(self, interval=None):\n",
    "        self.child = dict()\n",
    "        self.parent = dict()\n",
    "        self.interval = interval\n",
    "        self.id = f\"n{id(self)}\"\n",
    "        self.eda_expr = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.child.__len__()} children | {self.parent.__len__()} parents | {self.interval:.2f}\"\n",
    "    \n",
    "    def add_child(self, other, label):\n",
    "        self.child.update({label: other})\n",
    "        other.parent.update({label: self})\n",
    "\n",
    "    def pop_child(self, label):\n",
    "        child = self.child.pop(label)\n",
    "        child.parent.pop(label)\n",
    "        return child\n",
    "\n",
    "    def has_child(self):\n",
    "        return len(self.child) > 0\n",
    "\n",
    "class Interval:\n",
    "    def __init__(self, low, high, closed_left=True, closed_right=True):\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.left = closed_left\n",
    "        self.right = closed_right\n",
    "    \n",
    "    def __str__(self):\n",
    "        left = \"[\" if self.left else \"(\"\n",
    "        right = \"]\" if self.right else \")\"\n",
    "        return f\"{left}{self.low}; {self.high}{right}\"\n",
    "\n",
    "    def __format__(self, __format_spec):\n",
    "        left = \"[\" if self.left else \"(\"\n",
    "        right = \"]\" if self.right else \")\"\n",
    "        if __format_spec:\n",
    "            return f\"{left}{self.low:{__format_spec}}; {self.high:{__format_spec}}{right}\"\n",
    "        else:\n",
    "            return f\"{left}{self.low}; {self.high}{right}\"\n",
    "\n",
    "    def __add__(self, value):\n",
    "        return Interval(self.low + value, self.high + value, self.left, self.right)\n",
    "\n",
    "    def __sub__(self, value):\n",
    "        return Interval(self.low - value, self.high - value, self.left, self.right)\n",
    "\n",
    "    def __contains__(self, value):\n",
    "        down = value >= self.low if self.left else value > self.low\n",
    "        up = value <= self.high if self.right else value < self.high\n",
    "        return down & up\n",
    "    \n",
    "    def intersect(self, other):\n",
    "        if self.low < other.low:\n",
    "            self.low = other.low\n",
    "            self.left = other.left\n",
    "        elif self.low == other.low:\n",
    "            self.left &= other.left\n",
    "        \n",
    "        if self.high > other.high:\n",
    "            self.high = other.high\n",
    "            self.right = other.right\n",
    "        elif self.high == other.high:\n",
    "            self.right &= other.right\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = torch.where(input>=0, torch.tensor(1.0), torch.tensor(0.0))\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = torch.zeros_like(input)\n",
    "        return grad_input\n",
    "    \n",
    "class StepActivation(nn.Module):\n",
    "    def forward(self, input):\n",
    "        a =  StepFunction.apply(input)\n",
    "        return a\n",
    "    \n",
    "def layers2nn(layers):\n",
    "    a = []    \n",
    "    for x, y in zip(layers[:-1], layers[1:]):\n",
    "        a.append(nn.Linear(x, y))\n",
    "        a.append(StepActivation())\n",
    "    return nn.Sequential(*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(sig, frame):\n",
    "    raise Exception(\"Function takes too much time\")\n",
    "\n",
    "def timelimit(maxtime=100):\n",
    "    def inner(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            signal.signal(signal.SIGALRM, handler)\n",
    "            signal.alarm(maxtime)\n",
    "            try:\n",
    "                res = func(*args, **kwargs)\n",
    "            except Exception as exc:\n",
    "                print(exc)\n",
    "            else:\n",
    "                signal.alarm(-1)\n",
    "                return res\n",
    "        return wrapper\n",
    "    return inner\n",
    "\n",
    "def timecounter(message=None):\n",
    "    def inner(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start = time.time()\n",
    "            res = func(*args, **kwargs)\n",
    "            deltatime = (time.time() - start)*1000\n",
    "            if message:\n",
    "                try:\n",
    "                    print(message.format(deltatime))\n",
    "                except Exception:\n",
    "                    print(f\"Done in {deltatime}ms\")\n",
    "            elif message == False:\n",
    "                return res, deltatime\n",
    "            else:\n",
    "                print(f\"Done in {deltatime}ms\")\n",
    "            return res\n",
    "        return wrapper\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODD(ODD):\n",
    "    def clear_cache(self):\n",
    "        self.cache.clear()\n",
    "\n",
    "    def store_in_cache(self, k, node):\n",
    "        if self.cache.get(k):\n",
    "            self.cache[k].append(node)\n",
    "        else:\n",
    "            self.cache.update({k: [node]})\n",
    "\n",
    "    def find_in_cache(self, k, value):\n",
    "        cache_line = self.cache.get(k)\n",
    "        if cache_line:\n",
    "            for node in cache_line:\n",
    "                if value in node.interval:\n",
    "                    return node\n",
    "        return None\n",
    "\n",
    "    # TODO save count ? or count as we build it ?\n",
    "    def cache_total_node_count(self):\n",
    "        node_count = 0\n",
    "        for x in self.cache.values():\n",
    "            node_count+=x.__len__()\n",
    "        return node_count\n",
    "\n",
    "    def rebuild_cache(self, odd):\n",
    "        self.clear_cache()\n",
    "        self.cache[0] = [odd]\n",
    "        has_child = odd.has_child()\n",
    "        depth = 0\n",
    "        next_layer = set()\n",
    "        while has_child:\n",
    "            layer = self.cache[depth]\n",
    "            for node in layer:\n",
    "                for child in node.child.values():\n",
    "                    next_layer.add(child)\n",
    "\n",
    "            depth+=1\n",
    "            self.cache[depth] = list(next_layer)\n",
    "            next_layer.clear()\n",
    "            has_child = self.cache[depth][0].has_child()\n",
    "        \n",
    "        return self.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODD(ODD):\n",
    "    def make_graph(self):\n",
    "        if self.svg_graph is None:\n",
    "            self._make_graph()\n",
    "        return self.svg_graph\n",
    "    \n",
    "    def _make_graph(self):\n",
    "        dot = Digraph()\n",
    "        for cache_line in self.cache.values():\n",
    "            for node in cache_line:\n",
    "                dot.node(node.id, f\"{node.interval:.2f}\")\n",
    "                for e, child in node.child.items():\n",
    "                    dot.edge(node.id, child.id, str(e))\n",
    "\n",
    "        self.svg_graph = dot._repr_image_svg_xml()\n",
    "\n",
    "def str_bdd_tree(bdd):\n",
    "    def _build_str(node):\n",
    "        if node:\n",
    "            return f\"['root': {node.root}, 'hi': {_build_str(node.hi)}, 'lo': {_build_str(node.lo)}]\"\n",
    "    print(_build_str(bdd.node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ODD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODD(ODD):\n",
    "    @timecounter(message=False)\n",
    "    def timed_build_odd_rec(self, weights, threshold):\n",
    "        return self._build_odd_rec(weights, threshold)\n",
    "\n",
    "    def build_odd_rec(self, weights, threshold, label=\"\"):\n",
    "        self.weights = weights\n",
    "        self.label = label\n",
    "        self.root = self._build_odd_rec(weights, threshold)\n",
    "        self.bdd = self.root.eda_expr\n",
    "\n",
    "    @timelimit(600)\n",
    "    def _build_odd_rec(self, weights, threshold):\n",
    "        n = len(weights)\n",
    "        zero_sink = Node(Interval(-infty, threshold, closed_left=False, closed_right=False))\n",
    "        zero_sink.eda_expr = 0\n",
    "        self.zero = zero_sink\n",
    "        self.store_in_cache(n, zero_sink)\n",
    "\n",
    "        one_sink = Node(Interval(threshold, infty, closed_right=False))\n",
    "        one_sink.eda_expr = 1\n",
    "        self.store_in_cache(n, one_sink)\n",
    "        self.one = one_sink\n",
    "\n",
    "        return self.build_sub_odd_rec(0, 0)\n",
    "\n",
    "    def build_sub_odd_rec(self,  k, v):\n",
    "        node = Node(Interval(-infty, infty, closed_left=False, closed_right=False))\n",
    "        eda_var = inter.bddvar(f\"{self.label}{k}\")\n",
    "        self.eda_vars[eda_var] = self.svg_graph\n",
    "        weight = self.weights[k]\n",
    "        for e in {0, 1}:\n",
    "            w = e*weight\n",
    "            v_child = v + w\n",
    "            child = self.find_in_cache(k+1, v_child)\n",
    "            if child is None:\n",
    "                child = self.build_sub_odd_rec(k+1, v_child)\n",
    "            node.add_child(child, e)\n",
    "            node.interval.intersect(child.interval-w)\n",
    "        node.eda_expr = inter.ite(eda_var, node.child[1].eda_expr, node.child[0].eda_expr)\n",
    "        self.store_in_cache(k, node)\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers2odds(layers):\n",
    "    odds_layers = []\n",
    "    for i, layer in enumerate(layers):\n",
    "        odds = []\n",
    "        label = f\"l{i}_i\" if i else \"i\"\n",
    "        for weights, bias in zip(*layer):\n",
    "            odd = ODD()\n",
    "            odd.build_odd_rec(weights, -bias, label)\n",
    "            odds.append(odd)\n",
    "            odd.clear_cache()\n",
    "        odds_layers.append(odds)\n",
    "    return odds_layers\n",
    "\n",
    "def combine_odds(odds):\n",
    "    prev_layer = [(odd.bdd, odd.eda_vars) for odd in odds[0]]\n",
    "    p_vars = prev_layer[0][1]\n",
    "    for odds_next in odds[1:]:\n",
    "        next_layer = [(odd.bdd, odd.eda_vars) for odd in odds_next]\n",
    "        \n",
    "        switch_layer = []\n",
    "        for n_bdd, n_vars in next_layer:\n",
    "            res_bdd = n_bdd.compose({n_var: p_bdd for (p_bdd, _), n_var in zip(prev_layer, n_vars)}) \n",
    "            switch_layer.append((res_bdd, p_vars))\n",
    "\n",
    "        prev_layer = switch_layer\n",
    "    \n",
    "    return prev_layer[0]\n",
    "\n",
    "def compile_nn(net, verbose=False):\n",
    "    params = list(net.parameters())\n",
    "    if verbose:\n",
    "        print(\"converting to ODDs : \", end=\"\")\n",
    "        start_convert = time.perf_counter()\n",
    "    odds = layers2odds(zip(params[::2], params[1::2]))\n",
    "    if verbose:\n",
    "        print(f\"DONE ({time.perf_counter()-start_convert:1.2e})\\ncombining ODDs : \", end=\"\")\n",
    "        start_combine = time.perf_counter()\n",
    "    res = combine_odds(odds)\n",
    "    if verbose:\n",
    "        print(f\"DONE ({time.perf_counter()-start_combine:1.2e})\")\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO test limit case\n",
    "TEST = False\n",
    "\n",
    "def test_odd_build(n_weights):\n",
    "    neuron = nn.Linear(n_weights,1)\n",
    "\n",
    "    odd_test = ODD()\n",
    "    odd_test.build_odd_rec(neuron.weight[0], -neuron.bias[0])\n",
    "    for p in itertools.product([0, 1],repeat=n_weights):\n",
    "        s = 0\n",
    "        for w, e in zip(neuron.weight[0], p):\n",
    "            s+=w*e\n",
    "        eval_sum = s + neuron.bias[0] >= 0\n",
    "        eval_odd = bool(odd_test.root.eda_expr.restrict({k: v for k,v in zip(odd_test.eda_vars,p)}))\n",
    "        assert(eval_odd == eval_sum)\n",
    "    del odd_test\n",
    "\n",
    "def test_combine_odd(layers):\n",
    "    net = layers2nn(layers)\n",
    "    bdd, bdd_vars = compile_nn(net)\n",
    "    # try:\n",
    "    for p in itertools.product([0, 1],repeat=len(bdd_vars)):\n",
    "        eval_bdd = bdd.restrict({k: v for k,v in zip(bdd_vars,p)})\n",
    "        eval_bdd = bool(eval_bdd)\n",
    "        eval_net = bool(net.forward(torch.Tensor(p)))\n",
    "        assert(eval_bdd == eval_net)\n",
    "    # except AssertionError:\n",
    "    #     print(bdd, bdd_vars)\n",
    "    #     print(p, \"| bdd\", eval_bdd, \"| net\", eval_net)\n",
    "    #     params = list(net.parameters())\n",
    "    #     odds = layers2odds(zip(params[::2], params[1::2]))\n",
    "    #     print(params)\n",
    "    #     display(Source(bdd.to_dot()))\n",
    "    #     for x in odds:\n",
    "    #         print(\"_\"*30)\n",
    "    #         for odd in x:\n",
    "    #             display(Source(odd.bdd.to_dot()))\n",
    "    #     return True\n",
    "\n",
    "if TEST:\n",
    "    for n_weights in trange(1, 11): \n",
    "        for _ in range(100):\n",
    "            test_odd_build(n_weights)\n",
    "    \n",
    "    for _ in trange(1000):\n",
    "        if test_combine_odd([5,5,5,5,1]):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to ODDs : DONE (2.02e-01)\n",
      "combining ODDs : DONE (2.66e-06)\n"
     ]
    }
   ],
   "source": [
    "layers_nn = [15,1]\n",
    "\n",
    "net = layers2nn(layers_nn)\n",
    "bdd, bdd_vars = compile_nn(net, verbose=True)\n",
    "if layers_nn[0] < 10:\n",
    "    display(Source(bdd.to_dot()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approx BDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428\n",
      "432\n"
     ]
    }
   ],
   "source": [
    "def l(cmax, node):\n",
    "    return node.root if node.root > 0 else cmax+1\n",
    "\n",
    "memo_zp = dict()\n",
    "def zp(node, label):\n",
    "    a = memo_zp.get(node)\n",
    "    if a:\n",
    "        return a.get(label)\n",
    "    return None\n",
    "\n",
    "# pre compute zp by bottom-up algorithm\n",
    "def compute_zp(bdd, c):\n",
    "    for x in bdd.dfs_postorder():\n",
    "        if x.root < 0: # if leaf (0/1)\n",
    "            memo_zp[x] = {c: x}\n",
    "            continue\n",
    "        memo_zp[x] = {**memo_zp[x.lo], l(c, x): x}\n",
    "\n",
    "def init_find_incl(bdd, c):\n",
    "    R = []\n",
    "    nodes = {k:[] for k in range(1,c+2)}\n",
    "    for x in bdd.dfs_preorder():\n",
    "        R.append((x,x))\n",
    "        if x.root > 0:\n",
    "            nodes[x.root].append(x)\n",
    "        else:\n",
    "            nodes[c+1].append(x)\n",
    "    R.append((nodes[c+1][0], nodes[c+1][1]) if nodes[c+1][0] == -2 else (nodes[c+1][1], nodes[c+1][0]))\n",
    "\n",
    "    return nodes, R\n",
    "\n",
    "def find_incl(bdd, c):\n",
    "    nodes, R = init_find_incl(bdd, c)\n",
    "    print(len(R))\n",
    "    for k in range(c, 0, -1):\n",
    "        for u,v in itertools.combinations(nodes[k], 2):\n",
    "            if (u.lo, zp(v.lo, l(c, u.lo))) in R and (u.hi, zp(v.hi, l(c, u.hi))) in R:\n",
    "                R.append((u,v))\n",
    "            if (v.lo, zp(u.lo, l(c, v.lo))) in R and (v.hi, zp(u.hi, l(c, v.hi))) in R:\n",
    "                R.append((v, u))\n",
    "\n",
    "    print(len(R)) \n",
    "\n",
    "compute_zp(bdd, len(bdd_vars))\n",
    "find_incl(bdd,len(bdd_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<pyeda.boolalg.bdd.BDDNode object at 0x7f40318cfc10>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m compute_F(bdd_mono)\n\u001b[1;32m     72\u001b[0m compute_zp(bdd_mono, \u001b[38;5;28mlen\u001b[39m(bdd_vars_mono))\n\u001b[0;32m---> 73\u001b[0m \u001b[43mfind_incl_mono\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbdd_mono\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbdd_vars_mono\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 48\u001b[0m, in \u001b[0;36mfind_incl_mono\u001b[0;34m(bdd, c)\u001b[0m\n\u001b[1;32m     46\u001b[0m zp_q \u001b[38;5;241m=\u001b[39m zp(q, l(c, u))\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m u \u001b[38;5;129;01mis\u001b[39;00m zp_q:\n\u001b[0;32m---> 48\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[43msup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     49\u001b[0m     zp_q \u001b[38;5;241m=\u001b[39m zp(q, l(c, u))\n\u001b[1;32m     51\u001b[0m candidates\u001b[38;5;241m.\u001b[39madd(zp_q) \u001b[38;5;66;03m# zp can return None\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: <pyeda.boolalg.bdd.BDDNode object at 0x7f40318cfc10>"
     ]
    }
   ],
   "source": [
    "layers_nn_mono = [15, 1]\n",
    "\n",
    "while True:\n",
    "    net_mono = layers2nn(layers_nn_mono)\n",
    "    for i, params in enumerate(net_mono.parameters()):\n",
    "        if i%2==0:\n",
    "            params.data = abs(params.data)\n",
    "        else:\n",
    "            params.data = -10 * abs(params.data)\n",
    "\n",
    "    bdd_mono, bdd_vars_mono = compile_nn(net_mono)\n",
    "    if not bdd_mono.is_one() and not bdd_mono.is_zero():\n",
    "        break\n",
    "\n",
    "memo_F = dict()\n",
    "def compute_F(bdd): # ZDD algo -> adapt to BDD (?)\n",
    "    for x in bdd.dfs_postorder():\n",
    "        if x.root > 0:\n",
    "            memo_F[x] = {frozenset(S | {x.root}) for S in memo_F[x.hi]} | memo_F[x.lo]\n",
    "        elif x.root == -2: # case True\n",
    "            memo_F[x] = {frozenset()}\n",
    "        elif x.root == -1: # case False\n",
    "            memo_F[x] = set()\n",
    "\n",
    "def F(v):\n",
    "    return memo_F[v]\n",
    "\n",
    "def argmin(d):\n",
    "    return min(d, key=d.get)\n",
    "\n",
    "def find_incl_mono(bdd, c):\n",
    "    special_nodes = set(memo_zp[bdd.node].values()) # nodes reached by 0-edge from the root\n",
    "    nodes, _ = init_find_incl(bdd, c)\n",
    "    nodes[0] = dict() # simplify code for P0 and P1 computation\n",
    "    \n",
    "    sup = dict() # init ??\n",
    "\n",
    "    for k in range(1, c+1):\n",
    "        for u in set(nodes[k]) - special_nodes:\n",
    "            candidates = set()\n",
    "            P0 = {v for v in nodes[k-1] if v.lo is u}\n",
    "            P1 = {v for v in nodes[k-1] if v.hi is u}\n",
    "\n",
    "            for p in P0 | P1:\n",
    "                q = p\n",
    "                zp_q = zp(q, l(c, u))\n",
    "                while u is zp_q:\n",
    "                    q = sup[q]\n",
    "                    zp_q = zp(q, l(c, u))\n",
    "                \n",
    "                candidates.add(zp_q) # zp can return None\n",
    "                if zp_q is None:\n",
    "                    sys.exit(\"zp_q is None\")\n",
    "\n",
    "            for p in P1:\n",
    "                q = p\n",
    "                zp_q1 = zp(q.hi, l(c, u))\n",
    "                # print(zp_q1, q.hi.root, l(c,u), sup)\n",
    "                while u is zp_q1  and q not in special_nodes:\n",
    "                    q = sup[q]\n",
    "                    zp_q1 = zp(q.hi, l(c, u))\n",
    "                if u is not zp_q1:\n",
    "                    candidates.add(zp_q1) # zp_q1 can be None\n",
    "            \n",
    "            if candidates: \n",
    "                sup[u] = argmin({v: len(F(v)) for v in candidates}) # KeyError if None in candidates\n",
    "            \n",
    "    print(sup)\n",
    "\n",
    "memo_zp.clear()\n",
    "compute_F(bdd_mono)\n",
    "compute_zp(bdd_mono, len(bdd_vars_mono))\n",
    "find_incl_mono(bdd_mono, len(bdd_vars_mono))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
