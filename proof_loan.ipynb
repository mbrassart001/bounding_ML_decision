{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict \n",
    "from sklearn import metrics\n",
    "from torch.optim import Adam\n",
    "\n",
    "sys.path.append(os.path.abspath(''))\n",
    "\n",
    "import utils.more_torch_functions as mtf\n",
    "import datasets\n",
    "\n",
    "from utils.custom_loss import AsymBCELoss\n",
    "from utils.custom_activations import StepActivation\n",
    "from utils.modules import Parallel, MaxLayer\n",
    "from utils.misc import train_model, cov_score\n",
    "from compiling_nn.build_odd import compile_nn\n",
    "from compiling_nn.utils_odd import pickle_bdd, unpickle_bdd\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "SAVE_PATH = os.path.join(os.path.abspath(''), \"backup\")\n",
    "PKL_PATH = os.path.join(SAVE_PATH, \"bdd\")\n",
    "PTH_PATH = os.path.join(SAVE_PATH, \"nn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([296, 24])\n"
     ]
    }
   ],
   "source": [
    "np_x, np_y = datasets.LoanDataset.get_dataset(balancing=True, discretizing=True, hot_encoding=True)\n",
    "x_data, y_data = torch.Tensor(np_x), torch.Tensor(np_y)\n",
    "input_size = x_data.size(1)\n",
    "print(x_data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_outputs = {}\n",
    "def get_intermediate_outputs(name):\n",
    "    def hook(model, input, output):\n",
    "        if model.training:\n",
    "            intermediate_outputs.setdefault(name, dict())[\"train\"] = output\n",
    "        else:\n",
    "            intermediate_outputs.setdefault(name, dict())[\"valid\"] = output\n",
    "    return hook\n",
    "\n",
    "def true_label_for_backward(train, valid):\n",
    "    def hook(model, input):\n",
    "        if model.training:\n",
    "            model.true_labels = train\n",
    "        else:\n",
    "            model.true_labels = valid\n",
    "    return hook\n",
    "\n",
    "# créer hook fonction de perte pour meilleur backward ? (comparer individuellement les sorties des réseaux ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApproxNet(nn.Module):\n",
    "    hl1 = 10\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        hl1 = self.hl1\n",
    "\n",
    "        self.nn = nn.Sequential(OrderedDict([\n",
    "            ('l1', nn.Linear(input_size,hl1)),\n",
    "            ('a1', StepActivation()),\n",
    "            ('l2', nn.Linear(hl1,1)),\n",
    "            ('a2', StepActivation())\n",
    "        ]))        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.nn(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class BigNet(nn.Module):\n",
    "    hl1 = 50\n",
    "    hl2 = 25\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        hl1 = self.hl1\n",
    "        hl2 = self.hl2\n",
    "\n",
    "        self. nn = nn.Sequential(OrderedDict([\n",
    "            ('l1', nn.Linear(input_size,hl1)),\n",
    "            ('a1', nn.Sigmoid()),\n",
    "            ('l2', nn.Linear(hl1,hl2)),\n",
    "            ('a2', nn.Sigmoid()),\n",
    "            ('l3', nn.Linear(hl2,1)),\n",
    "            ('a3', StepActivation()),\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.nn(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('nets', Parallel(OrderedDict([\n",
    "                ('apx1', ApproxNet()),\n",
    "                ('apx2', ApproxNet()),\n",
    "                ('apx3', ApproxNet()),\n",
    "            ]))),\n",
    "            ('or_', MaxLayer()),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "    \n",
    "class BlankNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('nets', Parallel(OrderedDict([\n",
    "            ]))),\n",
    "            ('or_', MaxLayer()),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(x_train, y_train, x_valid, y_valid, model, criterion, optimizer):\n",
    "    train_model(x_train, y_train, model, criterion, optimizer, 500)\n",
    "    model.eval()\n",
    "    pred_train = model(x_train).detach()\n",
    "    pred_valid = model(x_valid).detach()\n",
    "\n",
    "    f1_train = metrics.f1_score(y_train, pred_train)\n",
    "    cov_train = cov_score(y_train, pred_train)\n",
    "    f1_valid = metrics.f1_score(y_valid, pred_valid)\n",
    "    cov_valid = cov_score(y_valid, pred_valid)\n",
    "\n",
    "    return f1_train, cov_train, f1_valid, cov_valid\n",
    "\n",
    "def print_eval(x_train, y_train, x_valid, y_valid, model, criterion, optimizer):\n",
    "    f1_train, cov_train, f1_valid, cov_valid = eval_model(x_train, y_train, x_valid, y_valid, model, criterion, optimizer)\n",
    "    print(\n",
    "        f\"{'':<15}{'Train':^15}{'Valid':^15}\",\n",
    "        f\"{'F1 score':<15}{f1_train:^15.3f}{f1_valid:^15.3f}\",\n",
    "        f\"{'Coverage [0]':<15}{cov_train[0]:^15.3f}{cov_valid[0]:^15.3f}\",\n",
    "        f\"{'         [1]':<15}{cov_train[1]:^15.3f}{cov_valid[1]:^15.3f}\",\n",
    "        sep=\"\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Train          Valid     \n",
      "F1 score            0.925          0.733     \n",
      "Coverage [0]        0.869          0.585     \n",
      "         [1]        0.980          0.787     \n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=1e-6)\n",
    "\n",
    "intermediate_outputs_handle = model.net.nets.register_forward_hook(get_intermediate_outputs(\"parallel_out\"))\n",
    "train_index, valid_index = torch.utils.data.random_split(range(x_data.size(0)), [0.7, 0.3])\n",
    "\n",
    "x_train, y_train = x_data[train_index], y_data[train_index]\n",
    "x_valid, y_valid = x_data[valid_index], y_data[valid_index]\n",
    "\n",
    "print_eval(x_train, y_train, x_valid, y_valid, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Train          Valid     \n",
      "F1 score            0.925          0.725     \n",
      "Coverage [0]        0.869          0.561     \n",
      "         [1]        0.980          0.787     \n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.net.nets.add_module(\"nn\", BigNet())\n",
    "optimizer = Adam(model.net.nets.nn.parameters(), lr=1e-2, weight_decay=1e-6)\n",
    "\n",
    "print_eval(x_train, y_train, x_valid, y_valid, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_outputs_handle.remove()\n",
    "torch.save(model, os.path.join(PTH_PATH, f\"3apx{ApproxNet.hl1}hl.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling apx1\n",
      "converting to ODDs : DONE (3.68e+02)\n",
      "combining ODDs : DONE (1.22e+03)\n",
      "compiling apx2\n",
      "converting to ODDs : DONE (5.00e+02)\n",
      "combining ODDs : DONE (1.95e+03)\n",
      "compiling apx3\n",
      "converting to ODDs : DONE (4.01e+02)\n",
      "combining ODDs : DONE (1.79e+03)\n"
     ]
    }
   ],
   "source": [
    "compiled_nn = []\n",
    "for name, approx in model.net.nets.named_children():\n",
    "    if name.startswith('apx'):\n",
    "        print(f\"compiling {name}\")\n",
    "\n",
    "        bdd = compile_nn(approx, verbose=True)\n",
    "        compiled_nn.append(bdd)\n",
    "        pickle_bdd(bdd, os.path.join(PKL_PATH, f\"{name}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, apx in enumerate(compiled_nn):\n",
    "#     pickle_bdd(apx, os.path.join(pkl_path, f\"apx{i+1}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i, apx in enumerate(compiled_nn):\n",
    "    unpickled_apx = unpickle_bdd(os.path.join(PKL_PATH, f\"apx{i+1}.pkl\"))\n",
    "    print(apx is unpickled_apx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "```md\n",
    "discrétisation -> méthode actuelle (ex: même chose : 400 -> [1,0,0,0] / 700 -> [0,1,0,0] / 1300 -> [0,0,1,0])\n",
    "               -> monotonie (ex: si groupé par 500 : 400 -> [1,0,0,0] / 700 -> [1,1,0,0] / 1300 -> [1,1,1,0])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
