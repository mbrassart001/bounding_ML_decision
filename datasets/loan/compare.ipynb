{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn import metrics, model_selection\n",
    "from torch.autograd import Function\n",
    "from torch.optim import Adam\n",
    "from graphviz import Source\n",
    "from pyeda.boolalg.bdd import bdd2expr\n",
    "\n",
    "filepath = os.path.abspath('')\n",
    "sys.path.append(os.path.join(filepath, \"..\", \"..\", \"compiling_nn\"))\n",
    "from build_odd import compile_nn\n",
    "\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm(y_true, y_pred):\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix, display_labels=[False, True])\n",
    "    return cm_display\n",
    "\n",
    "def plot_cm(y_true, y_pred):\n",
    "    cm_display = cm(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4,8))\n",
    "    cm_display.plot(ax=ax, colorbar=False)\n",
    "\n",
    "def plot_combine_cm(cms, titles=None):\n",
    "    n = len(cms)\n",
    "    fig, axs = plt.subplots(1, n, figsize=(4*n, 8))\n",
    "    if titles:\n",
    "        for ax, cm, title in zip(axs, cms, titles):\n",
    "            cm.plot(ax=ax, colorbar=False)\n",
    "            ax.set_title(title)\n",
    "    else:\n",
    "        for ax, cm in zip(axs, cms):\n",
    "            cm.plot(ax=ax, colorbar=False)\n",
    "    fig.tight_layout()\n",
    "\n",
    "def cov_score(y_true, y_pred):\n",
    "    labels = np.unique(y_true)\n",
    "    scores = {}\n",
    "\n",
    "    for label in labels:\n",
    "        indices_true = np.where(y_true == label)[0]\n",
    "        indices_pred = np.where(y_pred == label)[0]\n",
    "        scores[label] = len(np.intersect1d(indices_true, indices_pred))/len(indices_true)\n",
    "\n",
    "    return scores\n",
    "\n",
    "def cross_valid(X, Y, train_func, skf, **kw_train):\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        model = train_func(x_train, y_train, **kw_train)\n",
    "        model.eval()\n",
    "        yield model(x_test).detach(), y_test\n",
    "\n",
    "def tnot(a): return torch.logical_not(a)\n",
    "def tor(a,b): return torch.logical_or(a,b)\n",
    "def tand(a,b): return torch.logical_and(a,b)\n",
    "def txor(a,b): return torch.logical_xor(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = torch.where(input>=0, torch.tensor(1.0), torch.tensor(0.0))\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = torch.zeros_like(input)\n",
    "        return grad_input\n",
    "    \n",
    "class StepActivation(nn.Module):\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            return torch.sigmoid(input)\n",
    "        else:\n",
    "            return StepFunction.apply(input)\n",
    "    \n",
    "class AsymMSELoss(nn.Module): # https://www.desmos.com/calculator/zmxcluqhkt\n",
    "    def __init__(self, p=2):\n",
    "        super(AsymMSELoss, self).__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        dif = label - input\n",
    "        a = torch.square(dif)\n",
    "        b = a*self.p\n",
    "        loss = torch.where(dif < 0, b, a)\n",
    "        loss = torch.mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApproxNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        hl1 = 25\n",
    "\n",
    "        self.l1 = nn.Linear(25,hl1)\n",
    "        self.a1 = StepActivation()\n",
    "        self.l2 = nn.Linear(hl1,1)\n",
    "        self.a2 = StepActivation()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.a2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class CentralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        hl1 = 50\n",
    "        hl2 = 50\n",
    "\n",
    "        self. nn = nn.Sequential(\n",
    "            nn.Linear(25,hl1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hl1,hl2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hl2,1),\n",
    "            StepActivation(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.nn(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class BoundNetResults():\n",
    "    def __init__(self, x, xhi, xnn, xlo):\n",
    "        self.x = x\n",
    "        self.hi = xhi\n",
    "        self.nn = xnn\n",
    "        self.lo = xlo\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if hasattr(self.x, name):\n",
    "            return getattr(self.x, name)\n",
    "        else:\n",
    "            raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
    "    \n",
    "    def __dir__(self):\n",
    "        return dir(self.x)\n",
    "    \n",
    "    def detach(self):\n",
    "        self.x = self.x.detach()\n",
    "        return self\n",
    "\n",
    "class BoundNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # high approx nn (bigger = very long to compute into ODD)\n",
    "        self.hi = ApproxNet()\n",
    "\n",
    "        # low approx nn\n",
    "        self.lo = ApproxNet()\n",
    "\n",
    "        # nn to approximate (can make it bigger easily)\n",
    "        self.nn = CentralNet()\n",
    "\n",
    "    def forward(self, x):\n",
    "        xhi = self.hi(x)\n",
    "        xnn = self.nn(x)\n",
    "        xlo = self.lo(x)\n",
    "        xhir = torch.round(xhi)\n",
    "        xnnr = torch.round(xnn)\n",
    "        xlor = torch.round(xlo)\n",
    "        x = torch.where(xhi>0.5, xhi, torch.where(xlo<0.5, xlo, xnn))\n",
    "        # x = tor(tand(tnot(txor(xhir, xlor)), xhir), tand(txor(xhir, xlor), xnnr)).float()\n",
    "        x = BoundNetResults(x, xhi, xnn, xlo)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        hl1 = 50\n",
    "        hl2 = 50\n",
    "\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(25,hl1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hl1,hl2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hl2,1),\n",
    "            StepActivation(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.nn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_Status_Y\n",
      "0                184\n",
      "1                184\n",
      "dtype: int64\n",
      "torch.Size([368, 25]) torch.Size([368, 1])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"loan_data_set.csv\", sep=\",\")\n",
    "df = df.drop(columns=[\"Loan_ID\"])\n",
    "\n",
    "# Remove above 98.5th percentile for 'ApplicantIncome' and 'CoapplicantIncome'\n",
    "df_rank = df[[\"ApplicantIncome\", \"CoapplicantIncome\"]]\n",
    "df_rank[\"rankA\"] = df_rank[[\"ApplicantIncome\"]].rank(pct=True)\n",
    "df_rank[\"rankCo\"] = df_rank[\"CoapplicantIncome\"].rank(pct=True)\n",
    "\n",
    "df = df.loc[(df_rank[\"rankA\"]<=0.985) & (df_rank[\"rankCo\"]<=0.985)]\n",
    "df.index = range(len(df))\n",
    "\n",
    "# Transform using hot encoding\n",
    "df_y = pd.get_dummies(df[[\"Loan_Status\"]], drop_first=True)\n",
    "df_x = df.drop(columns=[\"Loan_Status\"])\n",
    "\n",
    "nunique = df_x.nunique(axis=0)\n",
    "df_x_mean = df_x.mean(axis=0, numeric_only=True)\n",
    "\n",
    "for col, n in nunique.items():\n",
    "    if n > 4:\n",
    "        df_x[col] = df_x[col].apply(lambda x : min(4, x//(.5*df_x_mean[col])))\n",
    "\n",
    "df_x = pd.get_dummies(df_x, columns=df_x.columns, drop_first=True)\n",
    "\n",
    "# Balance dataset\n",
    "itrue = df_y.index[df_y[\"Loan_Status_Y\"]==1].tolist()\n",
    "ifalse = df_y.index[df_y[\"Loan_Status_Y\"]==0].tolist()\n",
    "\n",
    "swap = len(itrue) > len(ifalse)\n",
    "if swap:\n",
    "    itrue,ifalse=ifalse,itrue\n",
    "\n",
    "ifalse = random.choices(ifalse, k=len(itrue))\n",
    "\n",
    "if swap:\n",
    "    itrue,ifalse=ifalse,itrue\n",
    "\n",
    "print(df_y.iloc[itrue+ifalse].value_counts())\n",
    "\n",
    "x_train=torch.Tensor(df_x.iloc[itrue+ifalse].to_numpy(dtype=int))\n",
    "y_train=torch.Tensor(df_y.iloc[itrue+ifalse].to_numpy(dtype=int))\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_boundnet(x, y, max_epoch=5000, learning_rate=1e-2):\n",
    "    boundnet = BoundNet()\n",
    "\n",
    "    # A loss function for each nn\n",
    "    loss_hi_nn = AsymMSELoss(100)\n",
    "    loss_lo_nn = AsymMSELoss(.001)\n",
    "    loss_nn_tr = nn.BCELoss()\n",
    "\n",
    "    # One optimizer for all parameters\n",
    "    optimizer = Adam(boundnet.parameters(), lr=learning_rate)\n",
    "\n",
    "    for _ in range(max_epoch):\n",
    "        boundnet.train()\n",
    "        y_pred = boundnet(x)\n",
    "        \n",
    "        # loss with true y values\n",
    "        loss_nn = loss_nn_tr(y_pred.nn, y)\n",
    "\n",
    "        # losses with y values of the trained nn\n",
    "        y_target = y_pred.nn.detach()\n",
    "        loss_hi = loss_hi_nn(y_pred.hi, y_target)\n",
    "        loss_lo = loss_lo_nn(y_pred.lo, y_target)\n",
    "\n",
    "        boundnet.zero_grad()\n",
    "        loss_nn.backward()\n",
    "        loss_hi.backward()\n",
    "        loss_lo.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return boundnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simplenet(x, y, max_epoch=5000, learning_rate=1e-2):\n",
    "    simplenet = SimpleNet()\n",
    "\n",
    "    # A loss function for each nn\n",
    "    loss_net = nn.BCELoss()\n",
    "\n",
    "    # One optimizer for all parameters\n",
    "    optimizer = Adam(simplenet.parameters(), lr=learning_rate)\n",
    "\n",
    "    for _ in range(max_epoch):\n",
    "        simplenet.train()\n",
    "        y_pred = simplenet(x_train)\n",
    "        \n",
    "        # loss in comparison with true y values\n",
    "        loss = loss_net(y_pred, y_train)\n",
    "\n",
    "        simplenet.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return simplenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "def show_activation(act, output):\n",
    "    # Mean activation per output\n",
    "    act_ones  = torch.where(output==1, act, torch.zeros(act.size()))\n",
    "    act_zeros = torch.where(output==0, act, torch.zeros(act.size()))\n",
    "\n",
    "    mean_ones  = torch.mean(act_ones, dim=0)\n",
    "    mean_zeros = torch.mean(act_zeros, dim=0)\n",
    "\n",
    "    # Figure initialization\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    tick_kw = {'left': False, 'bottom': False, 'labelleft': False}\n",
    "\n",
    "    # Normalize cmap accross both images\n",
    "    min_act = min(mean_ones.min().item(), mean_zeros.min().item())\n",
    "    max_act = max(mean_ones.max().item(), mean_zeros.max().item())\n",
    "\n",
    "    color_map = 'PRGn'\n",
    "\n",
    "    ax[0].imshow(mean_zeros.unsqueeze(0), cmap=color_map, vmin=min_act, vmax=max_act)\n",
    "    ax[0].tick_params(**tick_kw)\n",
    "    ax[0].set_title(\"activation moyenne de la couche cachée avec 0 en sortie\")\n",
    "\n",
    "    ax[1].imshow(mean_ones.unsqueeze(0), cmap=color_map, vmin=min_act, vmax=max_act)\n",
    "    ax[1].tick_params(**tick_kw)\n",
    "    ax[1].set_title(\"activation moyenne de la couche cachée avec 1 en sortie\")\n",
    "\n",
    "    # Show text on cells\n",
    "    for i, (v0, v1) in enumerate(zip(mean_zeros, mean_ones)):\n",
    "        ax[0].text(i, 0, f\"{v0.item():.2f}\", ha=\"center\", va=\"center\")\n",
    "        ax[1].text(i, 0, f\"{v1.item():.2f}\", ha=\"center\", va=\"center\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compute_activation(net, layer, data):\n",
    "    net.eval()\n",
    "    getattr(net, layer).register_forward_hook(get_activation('__net__'))\n",
    "    output = net(data).detach()\n",
    "    act = activation.pop('__net__').squeeze()\n",
    "    show_activation(act, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 :\n",
      "\tF1 score\tBoundNet\t0.700\t|\tSimpleNet\t0.950 \n",
      "\tCoverage\tHigh\t0.474\tLow\t0.000\n",
      "Fold 2 :\n",
      "\tF1 score\tBoundNet\t0.762\t|\tSimpleNet\t1.000 \n",
      "\tCoverage\tHigh\t0.684\tLow\t0.000\n",
      "Fold 3 :\n",
      "\tF1 score\tBoundNet\t0.714\t|\tSimpleNet\t1.000 \n",
      "\tCoverage\tHigh\t0.632\tLow\t0.333\n",
      "Fold 4 :\n",
      "\tF1 score\tBoundNet\t0.650\t|\tSimpleNet\t0.973 \n",
      "\tCoverage\tHigh\t0.526\tLow\t0.222\n",
      "Fold 5 :\n",
      "\tF1 score\tBoundNet\t0.600\t|\tSimpleNet\t0.971 \n",
      "\tCoverage\tHigh\t0.389\tLow\t0.211\n",
      "Fold 6 :\n",
      "\tF1 score\tBoundNet\t0.714\t|\tSimpleNet\t0.973 \n",
      "\tCoverage\tHigh\t0.611\tLow\t0.211\n",
      "Fold 7 :\n",
      "\tF1 score\tBoundNet\t0.571\t|\tSimpleNet\t1.000 \n",
      "\tCoverage\tHigh\t0.389\tLow\t0.263\n",
      "Fold 8 :\n",
      "\tF1 score\tBoundNet\t0.718\t|\tSimpleNet\t0.971 \n",
      "\tCoverage\tHigh\t0.611\tLow\t0.105\n",
      "Fold 9 :\n",
      "\tF1 score\tBoundNet\t0.595\t|\tSimpleNet\t0.973 \n",
      "\tCoverage\tHigh\t0.500\tLow\t0.000\n",
      "Fold 10 :\n",
      "\tF1 score\tBoundNet\t0.727\t|\tSimpleNet\t0.923 \n",
      "\tCoverage\tHigh\t0.500\tLow\t0.000\n"
     ]
    }
   ],
   "source": [
    "skf = model_selection.StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "bnet_split_res = cross_valid(x_train, y_train, train_boundnet, skf)\n",
    "snet_split_res = cross_valid(x_train, y_train, train_simplenet, skf)\n",
    "\n",
    "for i, ((b, t), (s, _)) in enumerate(zip(bnet_split_res, snet_split_res)):\n",
    "    b_f1_score = metrics.f1_score(t, b, average=\"binary\")\n",
    "    s_f1_score = metrics.f1_score(t, s, average=\"binary\")\n",
    "    hi_cov_score = cov_score(t, b.hi)[1]\n",
    "    lo_cov_score = cov_score(t, b.lo)[0]\n",
    "    print(f\"Fold {i+1} :\\n\\tF1 score\\tBoundNet\\t{b_f1_score:.3f}\\t|\\tSimpleNet\\t{s_f1_score:.3f}\",\n",
    "          f\"\\n\\tCoverage\\tHigh\\t{hi_cov_score:.3f}\\tLow\\t{lo_cov_score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
