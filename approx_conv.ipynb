{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict \n",
    "from sklearn import metrics, model_selection\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "sys.path.append(os.path.abspath(''))\n",
    "\n",
    "import datasets\n",
    "import utils.more_torch_functions as mtf\n",
    "\n",
    "from utils.custom_activations import StepActivation\n",
    "from utils.modules import Parallel, MaxLayer, MaxHierarchicalLayer\n",
    "from utils.misc import cross_valid, combine_prompts, cov_score, train_model\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11846, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "np_x, np_y = datasets.MnistDataset.get_dataset(balancing=True, keep_label=['0', '1'])\n",
    "x_data, y_data = torch.Tensor(np_x), torch.Tensor(np_y)\n",
    "input_size = x_data.size(1)\n",
    "print(x_data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_outputs = {}\n",
    "def get_intermediate_outputs(name):\n",
    "    def hook(model, input, output):\n",
    "        if model.training:\n",
    "            intermediate_outputs.setdefault(name, dict())[\"train\"] = output\n",
    "        else:\n",
    "            intermediate_outputs.setdefault(name, dict())[\"valid\"] = output\n",
    "    return hook\n",
    "\n",
    "def true_label_for_backward(train, valid):\n",
    "    def hook(model, input):\n",
    "        if model.training:\n",
    "            model.true_labels = train\n",
    "        else:\n",
    "            model.true_labels = valid\n",
    "    return hook\n",
    "\n",
    "# créer hook fonction de perte pour meilleur backward ? (comparer individuellement les sorties des réseaux ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApproxConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(1,1,3,1)),\n",
    "            ('a1', StepActivation()),\n",
    "            ('conv2', nn.Conv2d(1,1,3,1)),\n",
    "            ('a2', StepActivation()),\n",
    "            ('flatten', nn.Flatten()),\n",
    "        ]))\n",
    "\n",
    "        self.fc = nn.Sequential(OrderedDict([\n",
    "            ('fc', nn.Linear(24*24,1)),\n",
    "            ('afc', StepActivation()),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class CentralConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(1,1,3,1)),\n",
    "            ('a1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv2d(1,1,3,1)),\n",
    "            ('a2', nn.ReLU()),\n",
    "            ('conv3', nn.Conv2d(1,1,3,1)),\n",
    "            ('a3', nn.ReLU()),\n",
    "            ('flatten', nn.Flatten()),\n",
    "        ]))\n",
    "\n",
    "        self.fc = nn.Sequential(OrderedDict([\n",
    "            ('fc', nn.Linear(22*22,1)),\n",
    "            ('afc', StepActivation()),\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('nets', Parallel(OrderedDict([\n",
    "                ('cnn', CentralConvNet()),\n",
    "                ('apx1', ApproxConvNet()),\n",
    "            ]))),\n",
    "            ('or_', MaxLayer()),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32186/3036709081.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_images_tensor = torch.tensor(train_images, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
      "/tmp/ipykernel_32186/3036709081.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
      "/tmp/ipykernel_32186/3036709081.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_images_tensor = torch.tensor(val_images, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
      "/tmp/ipykernel_32186/3036709081.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_labels_tensor = torch.tensor(val_labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold [1/10], Epoch [1/10], Train Loss: 0.2373, Val Loss: 0.4112, Val Acc: 99.58%\n",
      "Fold [1/10], Epoch [2/10], Train Loss: 0.2342, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [1/10], Epoch [3/10], Train Loss: 0.2952, Val Loss: 0.4112, Val Acc: 99.58%\n",
      "Fold [1/10], Epoch [4/10], Train Loss: 0.2757, Val Loss: 0.1645, Val Acc: 99.83%\n",
      "Fold [1/10], Epoch [5/10], Train Loss: 0.1388, Val Loss: 0.1645, Val Acc: 99.83%\n",
      "Fold [1/10], Epoch [6/10], Train Loss: 0.1367, Val Loss: 0.1645, Val Acc: 99.83%\n",
      "Fold [1/10], Epoch [7/10], Train Loss: 0.1866, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [1/10], Epoch [8/10], Train Loss: 0.2161, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [1/10], Epoch [9/10], Train Loss: 0.0941, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [1/10], Epoch [10/10], Train Loss: 0.0945, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [2/10], Epoch [1/10], Train Loss: 0.0593, Val Loss: 0.3289, Val Acc: 99.66%\n",
      "Fold [2/10], Epoch [2/10], Train Loss: 0.0135, Val Loss: 0.4112, Val Acc: 99.58%\n",
      "Fold [2/10], Epoch [3/10], Train Loss: 0.0081, Val Loss: 0.2467, Val Acc: 99.75%\n",
      "Fold [2/10], Epoch [4/10], Train Loss: 0.0074, Val Loss: 0.3289, Val Acc: 99.66%\n",
      "Fold [2/10], Epoch [5/10], Train Loss: 0.0045, Val Loss: 0.2467, Val Acc: 99.75%\n",
      "Fold [2/10], Epoch [6/10], Train Loss: 0.0033, Val Loss: 0.2467, Val Acc: 99.75%\n",
      "Fold [2/10], Epoch [7/10], Train Loss: 0.0017, Val Loss: 0.2467, Val Acc: 99.75%\n",
      "Fold [2/10], Epoch [8/10], Train Loss: 0.0048, Val Loss: 0.1645, Val Acc: 99.83%\n",
      "Fold [2/10], Epoch [9/10], Train Loss: 0.0024, Val Loss: 1.2336, Val Acc: 98.73%\n",
      "Fold [2/10], Epoch [10/10], Train Loss: 0.0090, Val Loss: 0.4934, Val Acc: 99.49%\n",
      "Fold [3/10], Epoch [1/10], Train Loss: 0.0932, Val Loss: 0.1645, Val Acc: 99.83%\n",
      "Fold [3/10], Epoch [2/10], Train Loss: 0.0224, Val Loss: 0.1645, Val Acc: 99.83%\n",
      "Fold [3/10], Epoch [3/10], Train Loss: 0.0072, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [3/10], Epoch [4/10], Train Loss: 0.0036, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [3/10], Epoch [5/10], Train Loss: 0.0018, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [3/10], Epoch [6/10], Train Loss: 0.0011, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [3/10], Epoch [7/10], Train Loss: 0.0024, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [3/10], Epoch [8/10], Train Loss: 0.0028, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [3/10], Epoch [9/10], Train Loss: 0.0041, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [3/10], Epoch [10/10], Train Loss: 0.0031, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [4/10], Epoch [1/10], Train Loss: 0.8647, Val Loss: 0.8996, Val Acc: 99.16%\n",
      "Fold [4/10], Epoch [2/10], Train Loss: 0.6454, Val Loss: 0.1645, Val Acc: 99.83%\n",
      "Fold [4/10], Epoch [3/10], Train Loss: 0.3873, Val Loss: 0.2467, Val Acc: 99.75%\n",
      "Fold [4/10], Epoch [4/10], Train Loss: 0.5555, Val Loss: 0.1645, Val Acc: 99.83%\n",
      "Fold [4/10], Epoch [5/10], Train Loss: 0.2807, Val Loss: 0.4884, Val Acc: 99.58%\n",
      "Fold [4/10], Epoch [6/10], Train Loss: 0.2713, Val Loss: 0.4884, Val Acc: 99.58%\n",
      "Fold [4/10], Epoch [7/10], Train Loss: 0.2782, Val Loss: 0.4884, Val Acc: 99.58%\n",
      "Fold [4/10], Epoch [8/10], Train Loss: 0.2713, Val Loss: 0.4884, Val Acc: 99.58%\n",
      "Fold [4/10], Epoch [9/10], Train Loss: 0.2713, Val Loss: 0.4884, Val Acc: 99.58%\n",
      "Fold [4/10], Epoch [10/10], Train Loss: 0.2713, Val Loss: 0.4884, Val Acc: 99.58%\n",
      "Fold [5/10], Epoch [1/10], Train Loss: 0.4132, Val Loss: 33.9314, Val Acc: 67.00%\n",
      "Fold [5/10], Epoch [2/10], Train Loss: 0.3887, Val Loss: 33.7669, Val Acc: 67.17%\n",
      "Fold [5/10], Epoch [3/10], Train Loss: 0.3855, Val Loss: 33.7669, Val Acc: 67.17%\n",
      "Fold [5/10], Epoch [4/10], Train Loss: 0.3839, Val Loss: 33.6847, Val Acc: 67.26%\n",
      "Fold [5/10], Epoch [5/10], Train Loss: 0.3837, Val Loss: 33.8492, Val Acc: 67.09%\n",
      "Fold [5/10], Epoch [6/10], Train Loss: 0.3828, Val Loss: 33.5252, Val Acc: 67.34%\n",
      "Fold [5/10], Epoch [7/10], Train Loss: 0.3831, Val Loss: 33.5252, Val Acc: 67.34%\n",
      "Fold [5/10], Epoch [8/10], Train Loss: 0.3823, Val Loss: 33.6075, Val Acc: 67.26%\n",
      "Fold [5/10], Epoch [9/10], Train Loss: 0.3817, Val Loss: 33.4430, Val Acc: 67.43%\n",
      "Fold [5/10], Epoch [10/10], Train Loss: 0.3815, Val Loss: 33.4430, Val Acc: 67.43%\n",
      "Fold [6/10], Epoch [1/10], Train Loss: 0.4442, Val Loss: 0.8174, Val Acc: 99.24%\n",
      "Fold [6/10], Epoch [2/10], Train Loss: 0.2889, Val Loss: 0.4112, Val Acc: 99.58%\n",
      "Fold [6/10], Epoch [3/10], Train Loss: 2.9472, Val Loss: 3.6857, Val Acc: 96.46%\n",
      "Fold [6/10], Epoch [4/10], Train Loss: 4.5691, Val Loss: 1.0691, Val Acc: 98.90%\n",
      "Fold [6/10], Epoch [5/10], Train Loss: 0.8147, Val Loss: 0.4112, Val Acc: 99.58%\n",
      "Fold [6/10], Epoch [6/10], Train Loss: 0.2680, Val Loss: 0.4112, Val Acc: 99.58%\n",
      "Fold [6/10], Epoch [7/10], Train Loss: 0.2630, Val Loss: 0.4112, Val Acc: 99.58%\n",
      "Fold [6/10], Epoch [8/10], Train Loss: 0.2630, Val Loss: 0.4112, Val Acc: 99.58%\n",
      "Fold [6/10], Epoch [9/10], Train Loss: 0.2630, Val Loss: 0.4112, Val Acc: 99.58%\n",
      "Fold [6/10], Epoch [10/10], Train Loss: 0.2630, Val Loss: 0.4112, Val Acc: 99.58%\n",
      "Fold [7/10], Epoch [1/10], Train Loss: 0.3820, Val Loss: 22.2039, Val Acc: 78.72%\n",
      "Fold [7/10], Epoch [2/10], Train Loss: 0.3434, Val Loss: 21.9572, Val Acc: 78.89%\n",
      "Fold [7/10], Epoch [3/10], Train Loss: 0.3360, Val Loss: 21.6283, Val Acc: 79.22%\n",
      "Fold [7/10], Epoch [4/10], Train Loss: 0.3351, Val Loss: 21.7105, Val Acc: 79.14%\n",
      "Fold [7/10], Epoch [5/10], Train Loss: 0.3338, Val Loss: 21.4638, Val Acc: 79.39%\n",
      "Fold [7/10], Epoch [6/10], Train Loss: 0.3329, Val Loss: 21.4638, Val Acc: 79.39%\n",
      "Fold [7/10], Epoch [7/10], Train Loss: 0.3339, Val Loss: 21.5461, Val Acc: 79.31%\n",
      "Fold [7/10], Epoch [8/10], Train Loss: 0.3324, Val Loss: 21.4638, Val Acc: 79.39%\n",
      "Fold [7/10], Epoch [9/10], Train Loss: 0.3334, Val Loss: 21.3816, Val Acc: 79.48%\n",
      "Fold [7/10], Epoch [10/10], Train Loss: 0.3323, Val Loss: 21.3816, Val Acc: 79.48%\n",
      "Fold [8/10], Epoch [1/10], Train Loss: 0.2616, Val Loss: 1.2336, Val Acc: 98.73%\n",
      "Fold [8/10], Epoch [2/10], Train Loss: 0.0174, Val Loss: 0.5757, Val Acc: 99.41%\n",
      "Fold [8/10], Epoch [3/10], Train Loss: 0.0074, Val Loss: 0.2467, Val Acc: 99.75%\n",
      "Fold [8/10], Epoch [4/10], Train Loss: 0.0043, Val Loss: 0.2467, Val Acc: 99.75%\n",
      "Fold [8/10], Epoch [5/10], Train Loss: 0.0048, Val Loss: 0.1645, Val Acc: 99.83%\n",
      "Fold [8/10], Epoch [6/10], Train Loss: 0.0016, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [8/10], Epoch [7/10], Train Loss: 0.0003, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [8/10], Epoch [8/10], Train Loss: 0.0005, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [8/10], Epoch [9/10], Train Loss: 0.0002, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [8/10], Epoch [10/10], Train Loss: 0.0001, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [9/10], Epoch [1/10], Train Loss: 0.1972, Val Loss: 0.9046, Val Acc: 99.07%\n",
      "Fold [9/10], Epoch [2/10], Train Loss: 0.0107, Val Loss: 0.6579, Val Acc: 99.32%\n",
      "Fold [9/10], Epoch [3/10], Train Loss: 0.0059, Val Loss: 0.3289, Val Acc: 99.66%\n",
      "Fold [9/10], Epoch [4/10], Train Loss: 0.0029, Val Loss: 0.3289, Val Acc: 99.66%\n",
      "Fold [9/10], Epoch [5/10], Train Loss: 0.0019, Val Loss: 0.2467, Val Acc: 99.75%\n",
      "Fold [9/10], Epoch [6/10], Train Loss: 0.0030, Val Loss: 0.4112, Val Acc: 99.58%\n",
      "Fold [9/10], Epoch [7/10], Train Loss: 0.0021, Val Loss: 0.1645, Val Acc: 99.83%\n",
      "Fold [9/10], Epoch [8/10], Train Loss: 0.0003, Val Loss: 0.2467, Val Acc: 99.75%\n",
      "Fold [9/10], Epoch [9/10], Train Loss: 0.0001, Val Loss: 0.2467, Val Acc: 99.75%\n",
      "Fold [9/10], Epoch [10/10], Train Loss: 0.0000, Val Loss: 0.2467, Val Acc: 99.75%\n",
      "Fold [10/10], Epoch [1/10], Train Loss: 0.2163, Val Loss: 0.4112, Val Acc: 99.66%\n",
      "Fold [10/10], Epoch [2/10], Train Loss: 0.0141, Val Loss: 0.4112, Val Acc: 99.66%\n",
      "Fold [10/10], Epoch [3/10], Train Loss: 0.0060, Val Loss: 0.0822, Val Acc: 99.92%\n",
      "Fold [10/10], Epoch [4/10], Train Loss: 0.0057, Val Loss: 0.0000, Val Acc: 100.00%\n",
      "Fold [10/10], Epoch [5/10], Train Loss: 0.0044, Val Loss: 0.1645, Val Acc: 99.83%\n",
      "Fold [10/10], Epoch [6/10], Train Loss: 0.0012, Val Loss: 0.0000, Val Acc: 100.00%\n",
      "Fold [10/10], Epoch [7/10], Train Loss: 0.0011, Val Loss: 0.2467, Val Acc: 99.75%\n",
      "Fold [10/10], Epoch [8/10], Train Loss: 0.0003, Val Loss: 0.0000, Val Acc: 100.00%\n",
      "Fold [10/10], Epoch [9/10], Train Loss: 0.0000, Val Loss: 0.0000, Val Acc: 100.00%\n",
      "Fold [10/10], Epoch [10/10], Train Loss: 0.0000, Val Loss: 0.0000, Val Acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=1e-6)\n",
    "\n",
    "model.net.nets.register_forward_hook(get_intermediate_outputs(\"parallel_out\"))\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "num_splits = 10\n",
    "\n",
    "skf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=76)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(x_data, y_data)):\n",
    "    # Split data into train and validation sets\n",
    "    train_images, train_labels = x_data[train_index], y_data[train_index]\n",
    "    val_images, val_labels = x_data[val_index], y_data[val_index]\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    train_images_tensor = torch.tensor(train_images, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "    train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "    val_images_tensor = torch.tensor(val_images, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "    val_labels_tensor = torch.tensor(val_labels, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoader for training and validation sets\n",
    "    train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = TensorDataset(val_images_tensor, val_labels_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    mtf.reset_model(model)\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                total += labels.size(0)\n",
    "                correct += (outputs == labels).sum().item()\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        # Print statistics\n",
    "        print(f'Fold [{fold + 1}/{num_splits}], Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "              f'Val Loss: {val_loss / len(val_loader):.4f}, '\n",
    "              f'Val Acc: {(correct / total) * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
